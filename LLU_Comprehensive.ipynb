{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/samvance/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/samvance/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/samvance/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /Users/samvance/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "from nameparser.parser import HumanName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dataframe\n",
    "import pandas as pd\n",
    "# read saved data = FALSE\n",
    "df=pd.DataFrame(columns=['Source','Case Number', 'Authors','Discipline', 'Geography', 'Industry', 'Summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = 'https://services.hbsp.harvard.edu/api/catalog/products/KEL876-PDF-ENG'\n",
    "# resp = requests.get(url)\n",
    "# j = resp.json()\n",
    "# case_data = j['data']['products'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PULLING AND ORGANIZING SELECTED CASE INFOMRATION FROM HARVARD'S ONLINE CASE LIBRARIES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for case_num in range (0 ,1100):\n",
    "    url = 'https://services.hbsp.harvard.edu/api/catalog/products/KEL'+ str(case_num) + '-PDF-ENG'\n",
    "    response = requests.get(url)\n",
    "    try:\n",
    "        j = response.json()\n",
    "        case_data = j['data']['products'][0]\n",
    "        source = case_data['copyRightHolderDisplayName']\n",
    "        case_number = case_data['coreId']\n",
    "        author = case_data['authorNames']\n",
    "        discipline = case_data['majorDiscipline']\n",
    "        summary = case_data['abstractDescription']\n",
    "        geography = case_data['geography']\n",
    "        industry = case_data['industry']\n",
    "        year = case_data['publicationDate']\n",
    "        \n",
    "        \n",
    "        i = df.shape[0]\n",
    "        df.loc[i, 'Source'] = source\n",
    "        df.loc[i, 'Case Number'] = case_number\n",
    "        df.loc[i, 'Authors'] = author\n",
    "        df.loc[i, 'Discipline'] = discipline \n",
    "        df.loc[i, 'Geography'] = geography\n",
    "        df.loc[i, 'Industry'] = industry\n",
    "        df.loc[i, 'Summary'] = summary\n",
    "        df.loc[i, 'Year'] = year\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "teach_note = df['Summary'].str.contains('Teaching note') | df['Summary'].str.contains('Teaching Note')\n",
    "df = df[~teach_note]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ORGANZING DATA FRAME TO FIT NECESSARY API FORMATS: SEPERATING MULTI-AUTHORS AND SPLITTING FIRST AND LAST NAMES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new data frame with split value columns \n",
    "df['Authors list'] = df[\"Authors\"].str.split(\", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# authors = pd.DataFrame(df['Author'].str.split(', ').tolist()).stack()\n",
    "# authors_df = authors.str.split(\" \", n = 1, expand = True) \n",
    "# authors_df.columns = ['First Name', 'Last Name']\n",
    "# df.index.name='Case Number'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split multiple authors into separate columns\n",
    "authors_split = pd.DataFrame(df['Authors list'].tolist())\n",
    "# Stack authors into single column, one row per author (preserving multi-index)\n",
    "authors_stacked = authors_split.stack()\n",
    "# Split into first name and last name\n",
    "authors_stacked_first_last = authors_stacked.str.split(\" \", n = 1, expand = True)\n",
    "authors_stacked_first_last.columns = [\"First Name\",\"Last Name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/samvance/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "from ethnicolr import pred_wiki_name\n",
    "authors_with_race = pred_wiki_name(authors_stacked_first_last, \"Last Name\", \"First Name\")\n",
    "#Combine race df with complete df as dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gender_api(names_list):\n",
    "    import json\n",
    "    from urllib.request import urlopen\n",
    "    genders = []\n",
    "    for name in names_list:\n",
    "        first_name = \"First Name\"\n",
    "        myKey = \"JsPdwBuUoRBKvUlZMM\"\n",
    "        url = \"https://gender-api.com/get?key=\" + myKey + \"&name=\" + first_name\n",
    "        response = urlopen(url)\n",
    "        decoded = response.read().decode('utf-8')\n",
    "        data = json.loads(decoded)\n",
    "        genders.append(data[\"gender\"])\n",
    "    return genders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('Authors-with-demographics.txt', sep='\\t') #uses tab separated format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse the stacking to combine authors into single row (multi-columns!)\n",
    "authors_with_race_split = authors_with_race.unstack(1)\n",
    "# Reverse column splitting to combine into single columns\n",
    "# First get the column headings (from the multi-index column type)\n",
    "column_headings = authors_with_race_split.columns.levels[0]\n",
    "# Then get just the relevant columns\n",
    "race_columns = column_headings[2:]\n",
    "for c in race_columns:\n",
    "    new_column = 'Authors '+c\n",
    "    temp = df[['Authors list']]\n",
    "    temp[new_column] = authors_with_race_split[c].values.tolist()\n",
    "    # Truncate lists to remove NaN values for non-existant co-authors\n",
    "    df[new_column] = temp.apply(lambda x: x[new_column][:len(x['Authors list'])], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Authors list'].to_csv('Authors-with-race.txt', sep='\\t') #uses tab separated format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PROTAGS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nameparser.parser import HumanName\n",
    "\n",
    "def get_human_names(text):\n",
    "    tokens = nltk.tokenize.word_tokenize(text)\n",
    "    pos = nltk.pos_tag(tokens)\n",
    "    sentt = nltk.ne_chunk(pos, binary = False)\n",
    "    person_list = []\n",
    "    person = []\n",
    "    name = \"\"\n",
    "    for subtree in sentt.subtrees(filter=lambda t: t.label() == 'PERSON'):\n",
    "        for leaf in subtree.leaves():\n",
    "            person.append(leaf[0])\n",
    "        if len(person) > 1: #avoid grabbing lone surnames\n",
    "            for part in person:\n",
    "                name += part + ' '\n",
    "            if name[:-1] not in person_list:\n",
    "                person_list.append(name[:-1])\n",
    "            name = ''\n",
    "        person = []\n",
    "\n",
    "    return (person_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Protags']= df['Summary'].apply(get_human_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new data frame with split value columns \n",
    "df['Protags'] = df['Protags'].apply(str).str.split(\", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split multiple protags into separate columns\n",
    "protags_split = pd.DataFrame(df['Protags'].tolist())\n",
    "# Stack protags into single column, one row per author (preserving multi-index)\n",
    "protags_stacked = protags_split.stack()\n",
    "# Split into first name and last name\n",
    "protags_stacked_first_last = protags_stacked.str.split(\" \", n = 1, expand = True)\n",
    "protags_stacked_first_last.columns = [\"Protag First\",\"Protag Last\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "protags_with_race = pred_wiki_name(protags_stacked_first_last, \"Last Name\", \"First Name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse the stacking to combine authors into single row (multi-columns!)\n",
    "protags_with_race_split = authors_with_race.unstack(1)\n",
    "# Reverse column splitting to combine into single columns\n",
    "# First get the column headings (from the multi-index column type)\n",
    "column_headings = protags_with_race_split.columns.levels[0]\n",
    "# Then get just the relevant columns\n",
    "race_columns = column_headings[2:]\n",
    "for c in race_columns:\n",
    "    new_column = 'Protags '+c\n",
    "    temp = df[['Protags list']]\n",
    "    temp[new_column] = protags_with_race_split[c].values.tolist()\n",
    "    # Truncate lists to remove NaN values for non-existant co-authors\n",
    "    df[new_column] = temp.apply(lambda x: x[new_column][:len(x['Protags list'])], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
